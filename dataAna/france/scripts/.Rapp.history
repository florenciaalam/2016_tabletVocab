minTrialN=9 #MINIMUM USABLE TRIALS TO BE INCLUDED IN OVERALL ANALYSES #
minTrialNtype=4  #minimum amount of trials for each type   #notice that this means we will never look at crossing lexical type x difficulty level#
#
extractVwithNA<-function(allkids,res_incl){#
#add kids with no response data so that table can be square#
#and then return the averaged parameter#
 	names(res_incl)<-c("id","x")#
	if(sum(!(allkids %in% as.character(res_incl$id)))>0){#
	  nakids=cbind(as.character(allkids [!(allkids %in% as.character(res_incl$id))]),NA)#
	  colnames(nakids)<-c("id","x")#
	  res_incl=rbind(res_incl,nakids)#
	} #
  res_incl = res_incl[order(res_incl $id),]		#
  as.numeric(as.character(res_incl$x))#
}#
#
library(xlsx)
dirlist = dir(path="../data")#
#
data=NULL#
for(thisdir in dirlist){#
	dir(paste("../data",thisdir,sep="/"),pattern='csv')->allcsv#
	for(thisf in allcsv){#
		print(thisf)#
		read.csv(paste("../data",thisdir,thisf,sep="/"))->thiscsv#
		data =rbind(data,cbind(thisdir,thisf,thiscsv)) #
		}#
}#
dim(data)  #
write.table(data,"../results/paper/data.txt",row.names=F,sep="\t")#
#*************************************************************************************************************************************#
sumtab=NULL#
for(thisuuid in levels(data$uuid)) sumtab=rbind(sumtab,cbind(thisuuid,sum(data$uuid==thisuuid),data[data$uuid==thisuuid,c("subject_id","config_profile","session_started_at","thisdir","thisf")][1,]))#
write.table(sumtab,"../results/paper/sumtabCROSSac.txt",row.names=F,quote=F,sep="\t")#
#several IDs have two sets of data!!#
names(table(sumtab$subject_id))[table(sumtab$subject_id)>1]
write.table(data,"../results/data.txt",row.names=F,sep="\t")#
#*************************************************************************************************************************************#
sumtab=NULL#
for(thisuuid in levels(data$uuid)) sumtab=rbind(sumtab,cbind(thisuuid,sum(data$uuid==thisuuid),data[data$uuid==thisuuid,c("subject_id","config_profile","session_started_at","thisdir","thisf")][1,]))#
write.table(sumtab,"../results/sumtabCROSSac.txt",row.names=F,quote=F,sep="\t")#
#several IDs have two sets of data!!#
names(table(sumtab$subject_id))[table(sumtab$subject_id)>1]
read.table("../results/data.txt",header=T)-> data#
#
#remove non-relevant data#
data=data[grep("C",data$level_name),   ]#
#
#clean up subject id#
data$subject_id=factor(data$uuid)#
#
#*$* THROUGOUT SCRIPT, WE WILL COMPOSE A RESULTS TABLE, STARTING FROM NOTHING:#
results=NULL#
#*$* add ID info to the results table#
results$id=sort(data$subject_id[data$trial_number_session==1])#
#
#*$* add backgound info to the results table#
results$school = data$thisdir[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
results$date = data$date[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
#*$*profile#
results$profile = data$config_profile[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
#### STEP 1: CLEAN UP#
## Take into acount only test trials #
data[-grep("Training", data $level_name),   ]-> data#
#
#clean up names of objects#
data$object_asked=gsub("_.*","",data$object_asked)#
#
dim(data) #4793 trials
PRELIMINARIES: LOAD PACKAGES, DECLARE FUNCTIONS, SET PARAMETERS#
minTrialN=9 #MINIMUM USABLE TRIALS TO BE INCLUDED IN OVERALL ANALYSES #
minTrialNtype=4  #minimum amount of trials for each type   #notice that this means we will never look at crossing lexical type x difficulty level#
#
extractVwithNA<-function(allkids,res_incl){#
#add kids with no response data so that table can be square#
#and then return the averaged parameter#
 	names(res_incl)<-c("id","x")#
	if(sum(!(allkids %in% as.character(res_incl$id)))>0){#
	  nakids=cbind(as.character(allkids [!(allkids %in% as.character(res_incl$id))]),NA)#
	  colnames(nakids)<-c("id","x")#
	  res_incl=rbind(res_incl,nakids)#
	} #
  res_incl = res_incl[order(res_incl $id),]		#
  as.numeric(as.character(res_incl$x))#
}#
#
library(xlsx)#
### END PRELIMINARIES#
#
#*************************************************************************************************************************************#
#### CSV DATA#
## Read in the CSV#
dirlist = dir(path="../data")#
#
data=NULL#
for(thisdir in dirlist){#
	dir(paste("../data",thisdir,sep="/"),pattern='csv')->allcsv#
	for(thisf in allcsv){#
		print(thisf)#
		read.csv(paste("../data",thisdir,thisf,sep="/"))->thiscsv#
		data =rbind(data,cbind(thisdir,thisf,thiscsv)) #
		}#
}#
dim(data)  #
write.table(data,"../results/data.txt",row.names=F,sep="\t")#
#*************************************************************************************************************************************#
sumtab=NULL#
for(thisuuid in levels(data$uuid)) sumtab=rbind(sumtab,cbind(thisuuid,sum(data$uuid==thisuuid),data[data$uuid==thisuuid,c("subject_id","config_profile","session_started_at","thisdir","thisf")][1,]))#
write.table(sumtab,"../results/sumtabCROSSac.txt",row.names=F,quote=F,sep="\t")#
#several IDs have two sets of data!!#
names(table(sumtab$subject_id))[table(sumtab$subject_id)>1]#
#
#*************************************************************************************************************************************#
#
read.table("../results/data.txt",header=T)-> data#
#
#remove non-relevant data#
data=data[grep("C",data$level_name),   ]#
#
#clean up subject id#
data$subject_id=factor(data$uuid)#
#
#*$* THROUGOUT SCRIPT, WE WILL COMPOSE A RESULTS TABLE, STARTING FROM NOTHING:#
results=NULL#
#*$* add ID info to the results table#
results$id=sort(data$subject_id[data$trial_number_session==1])#
#
#*$* add backgound info to the results table#
results$school = data$thisdir[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
results$date = data$date[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
#*$*profile#
results$profile = data$config_profile[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
#### STEP 1: CLEAN UP#
## Take into acount only test trials #
data[-grep("Training", data $level_name),   ]-> data#
#
#clean up names of objects#
data$object_asked=gsub("_.*","",data$object_asked)#
#
dim(data) #4793 trials
STEP 2: RESULTS COLLAPSING ACROSS TRIAL TYPES#
#*$*trials completed#
results$trials_completed=as.numeric(table(data$subject_id)) #
#
#hist(data$object_touched_at)#
#hist(data$object_touched_at[data $object_touched_at<9000])#
#
## Exclusion criteria#
#maximum time to answer#
data[data $object_touched_at<7000,]-> data#
#
#dim(data) #4537 trials #
#
#*$* trials attempted#
results$trials_attempted=as.numeric(table(data$subject_id))#
#
#apply minimum amount of trials over the whole experiment#
bbMinOK=names(table(data $subject_id)[table(data $subject_id)>minTrialN])#
data[data $subject_id %in% bbMinOK,]-> data#
#
#dim(data) #4529 trials#
### ANALYSIS ACCURACY#
#*$* percent correct (overall)#
results$pc=extractVwithNA(levels(data$subject_id),	aggregate(data $correct,by=list(data $subject_id),mean))#
###XXXXX####
#Calculate exclusion matrix  #
exclude = table(data$subject_id,data$correct) < minTrialNtype   #Apply minimum number of trials #
#
### ANALYSIS RESPONSE TIMES #
results$rt_corr=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$correct==1],by=list(data $subject_id[data$correct==1]),median))#
results$rt_corr[exclude[,"1"]]<-NA#
#
results$rt_incorr=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$correct==0],by=list(data $subject_id[data$correct==0]),median))#
results$rt_incorr[exclude[,"0"]]<-NA#
#*$* for reliability calculations, re-do proportion correct & RT corr from odd and even trials#
data$spl=NA#
for(each_child in levels(data$subject_id)) if(sum(data$subject_id==each_child) == length(rep(1:2,sum(data$subject_id==each_child)/2))) data$spl[data$subject_id==each_child]<-rep(1:2, sum(data$subject_id==each_child)/2) else data$spl[data$subject_id==each_child]<-c(rep(1:2, sum(data$subject_id==each_child)/2),1)#
#
results$pc_1=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$spl==1],by=list(data $subject_id[data$spl==1]),mean))#
results$pc_2=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$spl==2],by=list(data $subject_id[data$spl==2]),mean))#
#
results$rt_1=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$spl==1],by=list(data $subject_id[data$spl==1]),median))#
results$rt_2=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$spl==2],by=list(data $subject_id[data$spl==2]),median))#
#
#*$* Write in the number of left responses#
results$propLeft=as.numeric(as.character(table(data$subject_id,data$object_touched_position)[,1]))/results$trials_attempted#
results$bias<-ifelse(results$propLeft <.4 | results$propLeft>.6,1,0)  #this should be improved!!#
#
#### STEP 3: RESULTS SEPARATING DIFFERENT TRIAL TYPES#
#
# Read in ancillary table#
dif=read.xlsx("_scripts/Difficultes.xlsx",1)#
#
## Add useful info to the data sheet#
#classify trials by word type & difficulty level#
data$dif=NA#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="easy"])]<-"easy"#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="moderate"])]<-"moderate"#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="difficult"])]<-"difficult"#
data$dif=factor(data$dif)#
#
data$lex=NA#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="adj"])]<-"adj"#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="noun"])]<-"noun"#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="verb"])]<-"verb"#
#
#### by difficulty level#
#Ns#
results$N_easy=as.numeric(table(data$subject_id,data$dif)[,"easy"])#
results$N_mod=as.numeric(table(data$subject_id,data$dif)[,"moderate"])#
results$N_diff= as.numeric(table(data$subject_id,data$dif)[,"difficult"])#
#Calculate exclusion matrix#
exclude = table(data$subject_id,data$dif) < minTrialNtype   #Apply minimum number of trials #
#*$* percent correct#
results$pc_easy=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="easy"],by=list(data $subject_id[data$dif== "easy"]),mean))#
results$pc_easy[exclude[,"easy"]]<-NA#
#
results$pc_mod=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="moderate"],by=list(data $subject_id[data$dif== "moderate"]),mean))#
results$pc_mod[exclude[,"moderate"]]<-NA#
#
results$pc_diff=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="difficult"],by=list(data $subject_id[data$dif== "difficult"]),mean))#
results$pc_diff[exclude[,"difficult"]]<-NA#
#Calculate exclusion matrix#
exclude = table(data$subject_id[data$correct==1],data$dif[data$correct==1]) < minTrialNtype   #Apply minimum number of trials #
#
#*$* rt corr only#
results$rt_easy=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="easy" & data$correct==1],by=list(data $subject_id[data$dif== "easy" & data$correct==1]), median))#
results$rt_easy[exclude[,"easy"]]<-NA#
#
results$rt_mod=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="moderate" & data$correct==1],by=list(data $subject_id[data$dif== "moderate" & data$correct==1]), median))#
results$rt_mod[exclude[,"moderate"]]<-NA#
#
results$rt_diff=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="difficult" & data$correct==1],by=list(data $subject_id[data$dif== "difficult" & data$correct==1]),median))#
results$rt_diff[exclude[,"difficult"]]<-NA#
#### by lexical type#
#Ns#
results$N_noun=as.numeric(table(data$subject_id,data$lex)[,"noun"])#
results$N_adj=as.numeric(table(data$subject_id,data$lex)[,"adj"])#
results$N_verb=as.numeric(table(data$subject_id,data$lex)[,"verb"])#
#Calculate exclusion matrix#
exclude = table(data$subject_id,data$lex) < minTrialNtype#
#*$* percent correct#
results$pc_noun=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="noun"],by=list(data $subject_id[data$lex== "noun"]),mean))#
results$pc_noun[exclude[,"noun"]]<-NA#
#
results$pc_adj =extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="adj"],by=list(data $subject_id[data$lex== "adj"]),mean))#
results$pc_adj[exclude[,"adj"]]<-NA#
#
results$pc_verb =extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="verb"],by=list(data $subject_id[data$lex== "verb"]),mean))#
results$pc_verb[exclude[,"verb"]]<-NA#
#*$* rt corr only#
results$rt_noun=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="noun" & data$correct==1],by=list(data $subject_id[data$lex== "noun" & data$correct==1]), median))#
results$rt_noun[exclude[,"noun"]]<-NA#
#
results$rt_adj=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="adj" & data$correct==1],by=list(data $subject_id[data$lex== "adj" & data$correct==1]), median))#
results$rt_adj[exclude[,"adj"]]<-NA#
#
results$rt_verb=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="verb" & data$correct==1],by=list(data $subject_id[data$lex== "verb" & data$correct==1]),median))#
results$rt_verb[exclude[,"verb"]]<-NA#
#Squarify and write out#
resultsdf=data.frame(results)#
write.table(resultsdf,"../results/results.txt",row.names=F,quote=F,sep="\t")#
#
#And also write out the final data#
write.table(data,"../results/data_final.txt",row.names=F,quote=F,sep="\t")#
#
## add individual information#
read.table("../results/quest.txt",header=T)->quest#
read.table("../results/corresp.txt",header=T)->corresp#
#
merge(resultsdf,corresp,by.x="id",by.y="CSV")->resultsdf#
merge(resultsdf,quest[,1:4],by.x="Anon",by.y="CA")->resultsdf#
resultsdf$age=(as.Date(resultsdf$Date,"%d/%m/%Y") - as.Date(resultsdf$DOB,"%d/%m/%Y"))/365.25#
resultsdf<-resultsdf[resultsdf$Remove=="no",]#
resultsdf$ed=ifelse(resultsdf$edMom>7,"higher","lower")#
write.table(resultsdf,"../results/results.txt",row.names=F,sep="\t")
dif=read.xlsx("../data/Difficultes.xlsx",1)
classify trials by word type & difficulty level#
data$dif=NA#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="easy"])]<-"easy"#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="moderate"])]<-"moderate"#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="difficult"])]<-"difficult"#
data$dif=factor(data$dif)#
#
data$lex=NA#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="adj"])]<-"adj"#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="noun"])]<-"noun"#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="verb"])]<-"verb"#
#
#### by difficulty level#
#Ns#
results$N_easy=as.numeric(table(data$subject_id,data$dif)[,"easy"])#
results$N_mod=as.numeric(table(data$subject_id,data$dif)[,"moderate"])#
results$N_diff= as.numeric(table(data$subject_id,data$dif)[,"difficult"])#
#Calculate exclusion matrix#
exclude = table(data$subject_id,data$dif) < minTrialNtype   #Apply minimum number of trials #
#*$* percent correct#
results$pc_easy=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="easy"],by=list(data $subject_id[data$dif== "easy"]),mean))#
results$pc_easy[exclude[,"easy"]]<-NA#
#
results$pc_mod=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="moderate"],by=list(data $subject_id[data$dif== "moderate"]),mean))#
results$pc_mod[exclude[,"moderate"]]<-NA#
#
results$pc_diff=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="difficult"],by=list(data $subject_id[data$dif== "difficult"]),mean))#
results$pc_diff[exclude[,"difficult"]]<-NA#
#Calculate exclusion matrix#
exclude = table(data$subject_id[data$correct==1],data$dif[data$correct==1]) < minTrialNtype   #Apply minimum number of trials #
#
#*$* rt corr only#
results$rt_easy=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="easy" & data$correct==1],by=list(data $subject_id[data$dif== "easy" & data$correct==1]), median))#
results$rt_easy[exclude[,"easy"]]<-NA#
#
results$rt_mod=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="moderate" & data$correct==1],by=list(data $subject_id[data$dif== "moderate" & data$correct==1]), median))#
results$rt_mod[exclude[,"moderate"]]<-NA#
#
results$rt_diff=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="difficult" & data$correct==1],by=list(data $subject_id[data$dif== "difficult" & data$correct==1]),median))#
results$rt_diff[exclude[,"difficult"]]<-NA#
#### by lexical type#
#Ns#
results$N_noun=as.numeric(table(data$subject_id,data$lex)[,"noun"])#
results$N_adj=as.numeric(table(data$subject_id,data$lex)[,"adj"])#
results$N_verb=as.numeric(table(data$subject_id,data$lex)[,"verb"])#
#Calculate exclusion matrix#
exclude = table(data$subject_id,data$lex) < minTrialNtype#
#*$* percent correct#
results$pc_noun=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="noun"],by=list(data $subject_id[data$lex== "noun"]),mean))#
results$pc_noun[exclude[,"noun"]]<-NA#
#
results$pc_adj =extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="adj"],by=list(data $subject_id[data$lex== "adj"]),mean))#
results$pc_adj[exclude[,"adj"]]<-NA#
#
results$pc_verb =extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="verb"],by=list(data $subject_id[data$lex== "verb"]),mean))#
results$pc_verb[exclude[,"verb"]]<-NA#
#*$* rt corr only#
results$rt_noun=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="noun" & data$correct==1],by=list(data $subject_id[data$lex== "noun" & data$correct==1]), median))#
results$rt_noun[exclude[,"noun"]]<-NA#
#
results$rt_adj=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="adj" & data$correct==1],by=list(data $subject_id[data$lex== "adj" & data$correct==1]), median))#
results$rt_adj[exclude[,"adj"]]<-NA#
#
results$rt_verb=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="verb" & data$correct==1],by=list(data $subject_id[data$lex== "verb" & data$correct==1]),median))#
results$rt_verb[exclude[,"verb"]]<-NA#
#Squarify and write out#
resultsdf=data.frame(results)#
write.table(resultsdf,"../results/results.txt",row.names=F,quote=F,sep="\t")#
#
#And also write out the final data#
write.table(data,"../results/data_final.txt",row.names=F,quote=F,sep="\t")#
#
## add individual information#
read.table("../results/quest.txt",header=T)->quest#
read.table("../results/corresp.txt",header=T)->corresp#
#
merge(resultsdf,corresp,by.x="id",by.y="CSV")->resultsdf#
merge(resultsdf,quest[,1:4],by.x="Anon",by.y="CA")->resultsdf#
resultsdf$age=(as.Date(resultsdf$Date,"%d/%m/%Y") - as.Date(resultsdf$DOB,"%d/%m/%Y"))/365.25#
resultsdf<-resultsdf[resultsdf$Remove=="no",]#
resultsdf$ed=ifelse(resultsdf$edMom>7,"higher","lower")#
write.table(resultsdf,"../results/results.txt",row.names=F,sep="\t")
library(lme4)#
library(car)#
library(psych)
read.table("../results/results.txt",header=T)->data#
data$age.c=data$age-mean(data$age,na.rm=T)#
data$langdummy=data$lang=="monolingual"#
data$langdummy[is.na(data$langdummy)]<-F #LAIA, I think there is an issue with the bilingual category -- there should be an "other" (not mono, not bi)#
#
rtstackcor=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_corr","rt_incorr")]))#
names(rtstackcor)[6:7]<-c("rt","resp")#
pcstackdif=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("pc_easy","pc_mod","pc_diff")]))#
names(pcstackdif)[6:7]<-c("pc","difficulty")#
#
rtstackdif=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_easy","rt_mod","rt_diff")]))#
names(rtstackdif)[6:7]<-c("rt","difficulty")#
#
pcstacklex=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("pc_noun","pc_adj","pc_verb")]))#
names(pcstacklex)[6:7]<-c("pc","lexCat")#
#
rtstacklex=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_noun","rt_adj","rt_verb")]))#
names(rtstacklex)[6:7]<-c("rt","lexCat")#
#CHECKS#
#boxplot(data$age~data$ed)#
#need to add age to the models for sure#
#
### 	ANALYed START#
sink("../results/stats_results.txt")#
print(date())#
print(dim(data))#
print(summary(data))
print("# ** planned analysis 1 **	total number of trials completed")#
#print(summary(lm(trials_completed ~ed+age.c,data=data))) #no difference across groups, marginal ages #
print(summary(lm(trials_completed ~ed+age.c+langdummy,data=data))) #all NS #
#
#plot(trials_completed ~age.c,data=data)#
#
print("trials_completed, mean & SD")#
print(cbind(c("Higher","Lower"),round(cbind(#
aggregate(data $trials_completed ,by=list(data$ed),mean)$x,#
aggregate(data $trials_completed ,by=list(data$ed),sd)$x),#
	3)))#
print("trials_completed, range")#
print(aggregate(data $trials_completed ,by=list(data$ed),range))#
print("# ** planned analysis 2 **	total number of trials attempted")#
#summary(lm(trials_attempted ~ed+age.c,data=data)) #no difference across groups, sign fx of age #
summary(lm(trials_attempted ~ed+age.c+langdummy,data=data)) #no difference across groups, sign fx of age #
#
print("trials_attempted, mean & SD")#
print(cbind(c("Higher","Lower"),round(cbind(#
aggregate(data $trials_attempted ,by=list(data$ed),mean)$x,#
aggregate(data $trials_attempted ,by=list(data$ed),sd)$x),#
	3)))#
#
print("trials_attempted, range")#
print(aggregate(data $trials_attempted ,by=list(data$ed),range))#
#
print("trials_attempted, distribution")#
print(table(data $trials_attempted,data$ed))#
#
print("# ** planned analysis 3 **	%correct & RT per group")#
#print(summary(lm((pc - .5)~ed+age.c,data=data)))  #difference in percent correct by age but not ed#
#print(summary(lm(log(rt_corr)~ed+age.c,data=data)))  #no difference in RT#
print(summary(lm((pc - .5)~ed+age.c+langdummy,data=data)))  #difference in percent correct by age and language, but not ed#
print(summary(lm(log(rt_corr)~ed+age.c+langdummy,data=data)))  #no difference in RT#
print(summary(lm((pc - .5)~ed+age.c+langdummy+school,data=data)))  #difference in percent correct by age and language, but not ed#
print(summary(lm(log(rt_corr)~ed+age.c+langdummy+school,data=data)))  #no difference in RT#
print("# ** not planned **	RT per group * corr")#
mixedRTr=lmer(log(rt) ~ ed*resp+age.c + langdummy + (1|id),data= rtstackcor)  #marginal age & sig difficulty, but no ed & no interaction#
print(Anova(mixedRTr))#
summary(mixedRTr)#
print("# ** analysis 4 **	%correct & RT per group * difficulty (check - not sure if we declared it)#
")#
#mixedPCdif=lmer((pc - .5)~ed*difficulty+age.c + (1|id),data= pcstackdif) # difficulty, NO age, no interaction#
#print(Anova(mixedPCdif))#
#
mixedPCdif=lmer((pc - .5)~ed*difficulty+age.c + langdummy + (1|id),data= pcstackdif) #age & difficulty & langdummy, no interaction#
print(Anova(mixedPCdif))#
print(summary(mixedPCdif))#
#
mixedPCdif=lmer((pc - .5)~ed*difficulty+age.c + langdummy + school + (1|id),data= pcstackdif) #age & difficulty & langdummy, no interaction#
print(Anova(mixedPCdif))#
#mixedRTdif=lmer(log(rt) ~ ed*difficulty+age.c + (1|id),data= rtstackdif)  #marginal age & sig difficulty, but no ed & no interaction#
#print(Anova(mixedRTdif))#
#
mixedRTdif=lmer(log(rt) ~ ed*difficulty+age.c + langdummy + (1|id),data= rtstackdif)  #marginal age & sig difficulty, but no ed & no interaction#
print(Anova(mixedRTdif))#
print(summary(mixedRTdif))#
#
mixedRTdif=lmer(log(rt) ~ ed*difficulty+age.c + langdummy + school + (1|id),data= rtstackdif)  #marginal age & sig difficulty, but no ed & no interaction#
print(Anova(mixedRTdif))#
print("# ** analysis 5 **	compare different conceptualizations of ed (maternal education, etc.)")#
print("##TO DO")#
#
print("# ** analysis ** Differences by word type")#
#mixedPClex=lmer((pc - .5)~ed* lexCat +age.c + (1|id),data= pcstacklex) #age & lex cat, BUT no ed nor ed*lex interaction#
#print(Anova(mixedPClex))#
#
mixedPClex=lmer((pc - .5)~ed* lexCat +age.c + langdummy + (1|id),data= pcstacklex) #age & lex cat & dummylang, BUT no ed nor ed*lex interaction#
print(Anova(mixedPClex))#
print(summary(mixedPClex))#
#
mixedPClex=lmer((pc - .5)~ed* lexCat +age.c + langdummy + school + (1|id),data= pcstacklex) #age & lex cat & dummylang, BUT no ed nor ed*lex interaction#
print(Anova(mixedPClex))#
#
#mixedRTlex=lmer(log(rt) ~ ed* lexCat +age.c + (1|id),data= rtstacklex)  # lex cat, no ed or age and no interaction#
#print(Anova(mixedRTlex))#
#
mixedRTlex=lmer(log(rt) ~ ed* lexCat +age.c + langdummy+ (1|id),data= rtstacklex)  # lex cat & langdummy, no ed or age and no interaction#
print(Anova(mixedRTlex))#
print(summary(mixedRTlex))#
#
mixedRTlex=lmer(log(rt) ~ ed* lexCat +age.c + langdummy + school + (1|id),data= rtstacklex)  # lex cat & langdummy, no ed or age and no interaction#
print(Anova(mixedRTlex))#
#
print("# ** planned analysis **	Internal validity with cronbach alpha, using split trial") #
# (we had also planned for a retest reliability but we didn't include those extra 13 trials)#
# Santos, J. R. A. (1999). Cronbachs alpha: A tool for asedsing the reliability of scales. Journal of Extension, 37, 1–5.#
print(alpha(cbind(data$pc_1,data$pc_2)))#
print(alpha(cbind(data$rt_1,data$rt_2)))#
# ** minor analyses **	#
print("# Test potential presence of side bias")#
print(table(data$bias))  #19!!! out of 51 kids have a bias according to a crude measure that should be improved!!#
print("# ** NUTSHELL **	Spearman correlations")#
print((cor.test(data $trials_completed , data $edMom, method="spearman")))#
print((cor.test(data $trials_attempted , data $edMom, method="spearman")))#
print((cor.test(data $pc , data $edMom, method="spearman")))#
print((cor.test(log(data $rt_corr) , data $edMom, method="spearman")))#
sink()#
source("scripts/2B_genAllFigs.R")
source("../scripts/2B_genAllFigs.R")
source("2B_genAllFigs.R")
par(mar=c(2.5,2.5,1,1)+0.1,mgp=c(1.5,0.5,0))#
plot(pc1 ~pc2,data=data,pch=20,xlab="% correct odd",ylab='% correct even')#
abline(lm(pc1 ~ pc2,data=data)))
names(data)
plot(pc_1 ~pc_2,data=data,pch=20,xlab="% correct odd",ylab='% correct even')
abline(lm(pc_1 ~ pc_2,data=data)))
abline(lm(pc_1 ~ pc_2,data=data))
pdf("../results/reliab_pc.pdf",width=5,height=5)#
par(mar=c(2.5,2.5,1,1)+0.1,mgp=c(1.5,0.5,0))#
plot(pc_1 ~pc_2,data=data,pch=20,xlab="% correct odd",ylab='% correct even',xlim=c(0,1),ylim=c(0,1))#
abline(lm(pc_1 ~ pc_2,data=data))#
dev.off()
par(mar=c(2.5,2.5,1,1)+0.1,mgp=c(1.5,0.5,0))#
plot(rt_1 ~rt_2,data=data,pch=20,xlab="% correct odd",ylab='% correct even',xlim=c(0,1),ylim=c(0,1))#
abline(lm(rt_1 ~ rt_2,data=data))
plot(rt_1/1000 ~rt_2/1000,data=data,pch=20,xlab="RT odd",ylab='RT even')#
abline(lm(rt_1 ~ rt_2,data=data))
plot((rt_1/1000) ~(rt_2/1000),data=data,pch=20,xlab="RT odd",ylab='RT even')
plot(rt_1 ~rt_2,data=data,pch=20,xlab="RT odd",ylab='RT even')
abline(lm(rt_1 ~ rt_2,data=data))
pdf("../results/reliab_rt.pdf",width=5,height=5)#
par(mar=c(2.5,2.5,1,1)+0.1,mgp=c(1.5,0.5,0))#
plot(rt_1 ~rt_2,data=data,pch=20,xlab="RT odd",ylab='RT even',xlim=c(1000,4500),ylim=c(1000,4500))#
abline(lm(rt_1 ~ rt_2,data=data))#
dev.off()
sink()
cor.test(rt_1 ~ rt_2,data=data)
cor.test(data $rt_1 ~ data $rt_2)
cor.test(data $rt_1 , data $rt_2)
cor.test(data $rt_1 , data $rt_2)$est
plot(rt_1 ~rt_2,data=data,pch=20,xlab="RT odd",ylab='RT even',xlim=c(1000,4500),ylim=c(1000,4500))
text(round(cor.test(data $rt_1 , data $rt_2)$est,3),1000,4000)
text(1000,4000,round(cor.test(data $rt_1 , data $rt_2)$est,3))
text(4000,1000,paste("r=",round(cor.test(data $rt_1 , data $rt_2)$est,3)))
pdf("../results/reliab_pc.pdf",width=5,height=5)#
par(mar=c(2.5,2.5,1,1)+0.1,mgp=c(1.5,0.5,0))#
plot(pc_1 ~pc_2,data=data,pch=20,xlab="% correct odd",ylab='% correct even',xlim=c(0,1),ylim=c(0,1))#
abline(lm(pc_1 ~ pc_2,data=data))#
text(.7,.2,paste("r=",round(cor.test(data $rt_1 , data $rt_2)$est,3)))#
dev.off()#
#
pdf("../results/reliab_rt.pdf",width=5,height=5)#
par(mar=c(2.5,2.5,1,1)+0.1,mgp=c(1.5,0.5,0))#
plot(rt_1 ~rt_2,data=data,pch=20,xlab="RT odd",ylab='RT even',xlim=c(1000,4500),ylim=c(1000,4500))#
abline(lm(rt_1 ~ rt_2,data=data))#
text(4000,1000,paste("r=",round(cor.test(data $rt_1 , data $rt_2)$est,3)))#
dev.off()
read.table("../results/results.txt",header=T)->ac
read.table("~/Dropbox/tabletAna/_results/cm_results.txt",header=T)->cm
colnames(ac) %in% colnames(cm)
ac[,c(colnames(ac) %in% colnames(cm))]
ac[,c(colnames(ac) %in% colnames(cm))]->acsel
cm[,c(colnames(ac) %in% colnames(cm))]->cmsel
cm[,c(colnames(cm) %in% colnames(ac))]->cmsel
names(cmsel)
names(acsel)
cm[,names(acsel)]->cmsel
names(cmsel)
acsel == cmsel
for(thiscol in names(cmsel)) acsel[,thiscol] == cmsel [,thiscol]
for(thiscol in names(cmsel)) print(acsel[,thiscol] == cmsel [,thiscol])
for(thiscol in names(cmsel)) print(sum(acsel[,thiscol] == cmsel [,thiscol])/length(cmsel [,thiscol]))
for(thiscol in names(cmsel)) print(paste(thiscol,sum(acsel[,thiscol] == cmsel [,thiscol])/length(cmsel [,thiscol])))
for(thiscol in names(cmsel)) print(paste(thiscol,sum(as.character(acsel[,thiscol]) == as.character(cmsel [,thiscol]))/length(cmsel [,thiscol])))
cbind(acsel[,"pc"],cmsel[,"pc"])
cbind(acsel[,"rt_adj"],cmsel[,"rt_adj"])
cbind(acsel[,"pc_easy"],cmsel[,"pc_easy"])
cbind(acsel[,"pc_dif"],cmsel[,"pc_diff"])
cbind(acsel[,"pc_difficult"],cmsel[,"pc_difficult"])
cbind(acsel[,"pc_diff"],cmsel[,"pc_diff"])
R analysis on word comprehension on Mandy's app#
# Argentina#
#
# Base  Alex Cristia alecristia@gmail.com 2016-04-25#
# Last edit May 2016#
#
# This script carries out the main analyed declared in the paper and prints out both figures included in the paper and also others not included there #
#
### PRELIMINARIES: LOAD PACKAGES, DECLARE FUNCTIONS, SET PARAMETERS#
library(lme4)#
library(car)#
library(psych)#
#
r2d<-function(r){(2*abs(r))/sqrt(1-abs(r)^2)}	#
#
### END PRELIMINARIES#
#
# Read in & create tables to be used later#
read.table("../results/results.txt",header=T)->data#
data$age.c=data$age-mean(data$age,na.rm=T)#
data$langdummy=data$lang=="monolingual"#
data$langdummy[is.na(data$langdummy)]<-F #LAIA, I think there is an issue with the bilingual category -- there should be an "other" (not mono, not bi)#
#
rtstackcor=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_corr","rt_incorr")]))#
names(rtstackcor)[6:7]<-c("rt","resp")#
pcstackdif=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("pc_easy","pc_mod","pc_diff")]))#
names(pcstackdif)[6:7]<-c("pc","difficulty")#
#
rtstackdif=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_easy","rt_mod","rt_diff")]))#
names(rtstackdif)[6:7]<-c("rt","difficulty")#
#
pcstacklex=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("pc_noun","pc_adj","pc_verb")]))#
names(pcstacklex)[6:7]<-c("pc","lexCat")#
#
rtstacklex=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_noun","rt_adj","rt_verb")]))#
names(rtstacklex)[6:7]<-c("rt","lexCat")
names(data)
data$corr= data$trials_attempted*data$pc
data$incorr= data$trials_attempted - (data$trials_attempted*data$pc)
summary(data)
?glm
summary(glm(pc~ed+age.c+langdummy,data=data,family=binomial))
summary(glm(c(corr,incorr)~ed+age.c+langdummy,data=data,family=binomial))
summary(glm(pc~ed+age.c+langdummy,data=data,family=binomial,weight= trials_attempted))
mixedRTr=lmer(log(rt) ~ ed*resp+age.c + langdummy + (1|id),data= rtstackcor)  #marginal age & sig difficulty, but no ed & no interaction
summary(mixedRTr)
names(data)
mod1=glm(pc~ed+age.c+langdummy,data=data,family=binomial,weight= trials_attempted)
mod2=glm(pc~edMom+age.c+langdummy,data=data,family=binomial,weight= trials_attempted)
anova(mod1,mod2)
summary(mod1)
summary(mod2)
mod2=glm(pc~(edMom-mean(edMom))+age.c+langdummy,data=data,family=binomial,weight= trials_attempted)
minTrialN=9 #MINIMUM USABLE TRIALS TO BE INCLUDED IN OVERALL ANALYSES #
minTrialNtype=4  #minimum amount of trials for each type   #notice that this means we will never look at crossing lexical type x difficulty level#
#
extractVwithNA<-function(allkids,res_incl){#
#add kids with no response data so that table can be square#
#and then return the averaged parameter#
 	names(res_incl)<-c("id","x")#
	if(sum(!(allkids %in% as.character(res_incl$id)))>0){#
	  nakids=cbind(as.character(allkids [!(allkids %in% as.character(res_incl$id))]),NA)#
	  colnames(nakids)<-c("id","x")#
	  res_incl=rbind(res_incl,nakids)#
	} #
  res_incl = res_incl[order(res_incl $id),]		#
  as.numeric(as.character(res_incl$x))#
}#
#
library(xlsx)#
### END PRELIMINARIES#
#
#*************************************************************************************************************************************#
#### CSV DATA#
## Read in the CSV#
dirlist = dir(path="../data")#
#
data=NULL#
for(thisdir in dirlist){#
	dir(paste("../data",thisdir,sep="/"),pattern='csv')->allcsv#
	for(thisf in allcsv){#
		print(thisf)#
		read.csv(paste("../data",thisdir,thisf,sep="/"))->thiscsv#
		data =rbind(data,cbind(thisdir,thisf,thiscsv)) #
		}#
}#
dim(data)  #
write.table(data,"../results/data.txt",row.names=F,sep="\t")#
#*************************************************************************************************************************************#
sumtab=NULL#
for(thisuuid in levels(data$uuid)) sumtab=rbind(sumtab,cbind(thisuuid,sum(data$uuid==thisuuid),data[data$uuid==thisuuid,c("subject_id","config_profile","session_started_at","thisdir","thisf")][1,]))#
write.table(sumtab,"../results/sumtabCROSSac.txt",row.names=F,quote=F,sep="\t")#
#several IDs have two sets of data!!#
names(table(sumtab$subject_id))[table(sumtab$subject_id)>1]
read.table("../results/data.txt",header=T)-> data#
#
#remove non-relevant data#
data=data[grep("C",data$level_name),   ]#
#
#clean up subject id#
data$subject_id=factor(data$uuid)#
#
#*$* THROUGOUT SCRIPT, WE WILL COMPOSE A RESULTS TABLE, STARTING FROM NOTHING:#
results=NULL#
#*$* add ID info to the results table#
results$id=sort(data$subject_id[data$trial_number_session==1])#
#
#*$* add backgound info to the results table#
results$school = data$thisdir[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
results$date = data$date[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
#*$*profile#
results$profile = data$config_profile[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
#### STEP 1: CLEAN UP#
## Take into acount only test trials #
data[-grep("Training", data $level_name),   ]-> data#
#
#clean up names of objects#
#data$object_asked=gsub("_.*","",data$object_asked)#
#
dim(data) #4793 trials #
#why am I not getting the same number???#
#### STEP 2: RESULTS COLLAPSING ACROSS TRIAL TYPES#
#*$*trials completed#
results$trials_completed=as.numeric(table(data$subject_id)) #
#
#hist(data$object_touched_at)#
#hist(data$object_touched_at[data $object_touched_at<9000])#
#
## Exclusion criteria#
#maximum time to answer#
data[data $object_touched_at<7000,]-> data#
#
#dim(data) #4537 trials #
#
#*$* trials attempted#
results$trials_attempted=as.numeric(table(data$subject_id))#
#
#apply minimum amount of trials over the whole experiment#
bbMinOK=names(table(data $subject_id)[table(data $subject_id)>minTrialN])#
data[data $subject_id %in% bbMinOK,]-> data#
#
#dim(data) #4529 trials
#*$* percent correct (overall)#
results$pc=extractVwithNA(levels(data$subject_id),	aggregate(data $correct,by=list(data $subject_id),mean))#
###XXXXX####
#Calculate exclusion matrix  #
exclude = table(data$subject_id,data$correct) < minTrialNtype   #Apply minimum number of trials #
#
### ANALYSIS RESPONSE TIMES #
results$rt_corr=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$correct==1],by=list(data $subject_id[data$correct==1]),median))#
results$rt_corr[exclude[,"1"]]<-NA#
#
results$rt_incorr=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$correct==0],by=list(data $subject_id[data$correct==0]),median))#
results$rt_incorr[exclude[,"0"]]<-NA#
#*$* for reliability calculations, re-do proportion correct & RT corr from odd and even trials#
data$spl=NA#
for(each_child in levels(data$subject_id)) if(sum(data$subject_id==each_child) == length(rep(1:2,sum(data$subject_id==each_child)/2))) data$spl[data$subject_id==each_child]<-rep(1:2, sum(data$subject_id==each_child)/2) else data$spl[data$subject_id==each_child]<-c(rep(1:2, sum(data$subject_id==each_child)/2),1)#
#
results$pc_1=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$spl==1],by=list(data $subject_id[data$spl==1]),mean))#
results$pc_2=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$spl==2],by=list(data $subject_id[data$spl==2]),mean))#
#
results$rt_1=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$spl==1],by=list(data $subject_id[data$spl==1]),median))#
results$rt_2=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$spl==2],by=list(data $subject_id[data$spl==2]),median))#
#
#*$* Write in the number of left responses#
results$propLeft=as.numeric(as.character(table(data$subject_id,data$object_touched_position)[,1]))/results$trials_attempted#
results$bias<-ifelse(results$propLeft <.4 | results$propLeft>.6,1,0)  #this should be improved!!#
#
#### STEP 3: RESULTS SEPARATING DIFFERENT TRIAL TYPES#
#
# Read in ancillary table#
dif=read.xlsx("../data/Difficultes.xlsx",1)#
#
## Add useful info to the data sheet#
#classify trials by word type & difficulty level#
data$dif=NA#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="easy"])]<-"easy"#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="moderate"])]<-"moderate"#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="difficult"])]<-"difficult"#
data$dif=factor(data$dif)#
#
data$lex=NA#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="adj"])]<-"adj"#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="noun"])]<-"noun"#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="verb"])]<-"verb"#
#
#### by difficulty level#
#Ns#
results$N_easy=as.numeric(table(data$subject_id,data$dif)[,"easy"])#
results$N_mod=as.numeric(table(data$subject_id,data$dif)[,"moderate"])#
results$N_diff= as.numeric(table(data$subject_id,data$dif)[,"difficult"])#
#Calculate exclusion matrix#
exclude = table(data$subject_id,data$dif) < minTrialNtype   #Apply minimum number of trials #
#*$* percent correct#
results$pc_easy=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="easy"],by=list(data $subject_id[data$dif== "easy"]),mean))#
results$pc_easy[exclude[,"easy"]]<-NA#
#
results$pc_mod=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="moderate"],by=list(data $subject_id[data$dif== "moderate"]),mean))#
results$pc_mod[exclude[,"moderate"]]<-NA#
#
results$pc_diff=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="difficult"],by=list(data $subject_id[data$dif== "difficult"]),mean))#
results$pc_diff[exclude[,"difficult"]]<-NA#
#Calculate exclusion matrix#
exclude = table(data$subject_id[data$correct==1],data$dif[data$correct==1]) < minTrialNtype   #Apply minimum number of trials #
#
#*$* rt corr only#
results$rt_easy=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="easy" & data$correct==1],by=list(data $subject_id[data$dif== "easy" & data$correct==1]), median))#
results$rt_easy[exclude[,"easy"]]<-NA#
#
results$rt_mod=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="moderate" & data$correct==1],by=list(data $subject_id[data$dif== "moderate" & data$correct==1]), median))#
results$rt_mod[exclude[,"moderate"]]<-NA#
#
results$rt_diff=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="difficult" & data$correct==1],by=list(data $subject_id[data$dif== "difficult" & data$correct==1]),median))#
results$rt_diff[exclude[,"difficult"]]<-NA#
#### by lexical type#
#Ns#
results$N_noun=as.numeric(table(data$subject_id,data$lex)[,"noun"])#
results$N_adj=as.numeric(table(data$subject_id,data$lex)[,"adj"])#
results$N_verb=as.numeric(table(data$subject_id,data$lex)[,"verb"])#
#Calculate exclusion matrix#
exclude = table(data$subject_id,data$lex) < minTrialNtype#
#*$* percent correct#
results$pc_noun=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="noun"],by=list(data $subject_id[data$lex== "noun"]),mean))#
results$pc_noun[exclude[,"noun"]]<-NA#
#
results$pc_adj =extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="adj"],by=list(data $subject_id[data$lex== "adj"]),mean))#
results$pc_adj[exclude[,"adj"]]<-NA#
#
results$pc_verb =extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="verb"],by=list(data $subject_id[data$lex== "verb"]),mean))#
results$pc_verb[exclude[,"verb"]]<-NA#
#*$* rt corr only#
results$rt_noun=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="noun" & data$correct==1],by=list(data $subject_id[data$lex== "noun" & data$correct==1]), median))#
results$rt_noun[exclude[,"noun"]]<-NA#
#
results$rt_adj=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="adj" & data$correct==1],by=list(data $subject_id[data$lex== "adj" & data$correct==1]), median))#
results$rt_adj[exclude[,"adj"]]<-NA#
#
results$rt_verb=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="verb" & data$correct==1],by=list(data $subject_id[data$lex== "verb" & data$correct==1]),median))#
results$rt_verb[exclude[,"verb"]]<-NA
resultsdf=data.frame(results)
write.table(resultsdf,"../results/results.txt",row.names=F,quote=F,sep="\t")
write.table(data,"../results/data_final.txt",row.names=F,quote=F,sep="\t")
read.table("../results/quest.txt",header=T)->quest#
read.table("../results/corresp.txt",header=T)->corresp
merge(resultsdf,corresp,by.x="id",by.y="CSV")->resultsdf#
merge(resultsdf,quest[,1:4],by.x="Anon",by.y="CA")->resultsdf#
resultsdf$age=(as.Date(resultsdf$Date,"%d/%m/%Y") - as.Date(resultsdf$DOB,"%d/%m/%Y"))/365.25#
resultsdf<-resultsdf[resultsdf$Remove=="no",]#
resultsdf$ed=ifelse(resultsdf$edMom>7,"higher","lower")
write.table(resultsdf,"../results/results.txt",row.names=F,sep="\t")
library(lme4)#
library(car)#
library(psych)#
#
r2d<-function(r){(2*abs(r))/sqrt(1-abs(r)^2)}
read.table("../results/results.txt",header=T)->data#
data$age.c=data$age-mean(data$age,na.rm=T)#
data$langdummy=data$lang=="monolingual"#
data$langdummy[is.na(data$langdummy)]<-F #LAIA, I think there is an issue with the bilingual category -- there should be an "other" (not mono, not bi)#
#
rtstackcor=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_corr","rt_incorr")]))#
names(rtstackcor)[6:7]<-c("rt","resp")#
pcstackdif=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("pc_easy","pc_mod","pc_diff")]))#
names(pcstackdif)[6:7]<-c("pc","difficulty")#
#
rtstackdif=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_easy","rt_mod","rt_diff")]))#
names(rtstackdif)[6:7]<-c("rt","difficulty")#
#
pcstacklex=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("pc_noun","pc_adj","pc_verb")]))#
names(pcstacklex)[6:7]<-c("pc","lexCat")#
#
rtstacklex=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_noun","rt_adj","rt_verb")]))#
names(rtstacklex)[6:7]<-c("rt","lexCat")#
#CHECKS
head(data)
rtstackcor=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_corr","rt_incorr")]))#
names(rtstackcor)[6:7]<-c("rt","resp")
names(data)
data$langdummy=data$lang=="monolingual"#
data$langdummy[is.na(data$langdummy)]<-F #LAIA, I think there is an issue with the
data$langdummy=="monolingual"
data$langdummy = data$lang=="monolingual"
data$lang
names(quest)
read.table("../results/results.txt",header=T)-> resultsdf
# R analysis on word comprehension on Mandy's app#
# France#
#
# Base  Alex Cristia alecristia@gmail.com 2016-04-25#
# Last edit Jun 2016#
#
# This script puts together the dataset that will be analyzed in subsequent scripts#
#
### PRELIMINARIES: LOAD PACKAGES, DECLARE FUNCTIONS, SET PARAMETERS#
minTrialN=9 #MINIMUM USABLE TRIALS TO BE INCLUDED IN OVERALL ANALYSES #
minTrialNtype=4  #minimum amount of trials for each type   #notice that this means we will never look at crossing lexical type x difficulty level#
#
extractVwithNA<-function(allkids,res_incl){#
#add kids with no response data so that table can be square#
#and then return the averaged parameter#
 	names(res_incl)<-c("id","x")#
	if(sum(!(allkids %in% as.character(res_incl$id)))>0){#
	  nakids=cbind(as.character(allkids [!(allkids %in% as.character(res_incl$id))]),NA)#
	  colnames(nakids)<-c("id","x")#
	  res_incl=rbind(res_incl,nakids)#
	} #
  res_incl = res_incl[order(res_incl $id),]		#
  as.numeric(as.character(res_incl$x))#
}#
#
library(xlsx)#
### END PRELIMINARIES#
#
#*************************************************************************************************************************************#
#### CSV DATA#
## Read in the CSV#
dirlist = dir(path="../data")#
#
data=NULL#
for(thisdir in dirlist){#
	dir(paste("../data",thisdir,sep="/"),pattern='csv')->allcsv#
	for(thisf in allcsv){#
		print(thisf)#
		read.csv(paste("../data",thisdir,thisf,sep="/"))->thiscsv#
		data =rbind(data,cbind(thisdir,thisf,thiscsv)) #
		}#
}#
dim(data)  #
write.table(data,"../results/data.txt",row.names=F,sep="\t")#
#*************************************************************************************************************************************#
sumtab=NULL#
for(thisuuid in levels(data$uuid)) sumtab=rbind(sumtab,cbind(thisuuid,sum(data$uuid==thisuuid),data[data$uuid==thisuuid,c("subject_id","config_profile","session_started_at","thisdir","thisf")][1,]))#
write.table(sumtab,"../results/sumtabCROSSac.txt",row.names=F,quote=F,sep="\t")#
#several IDs have two sets of data!!#
names(table(sumtab$subject_id))[table(sumtab$subject_id)>1]#
#
#*************************************************************************************************************************************#
#
read.table("../results/data.txt",header=T)-> data#
#
#remove non-relevant data#
data=data[grep("C",data$level_name),   ]#
#
#clean up subject id#
data$subject_id=factor(data$uuid)#
#
#*$* THROUGOUT SCRIPT, WE WILL COMPOSE A RESULTS TABLE, STARTING FROM NOTHING:#
results=NULL#
#*$* add ID info to the results table#
results$id=sort(data$subject_id[data$trial_number_session==1])#
#
#*$* add backgound info to the results table#
results$school = data$thisdir[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
results$date = data$date[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
#*$*profile#
results$profile = data$config_profile[data$trial_number_session==1][order(data$subject_id[data$trial_number_session==1])]#
#### STEP 1: CLEAN UP#
## Take into acount only test trials #
data[-grep("Training", data $level_name),   ]-> data#
#
#clean up names of objects#
#data$object_asked=gsub("_.*","",data$object_asked)#
#
dim(data) #4793 trials #
#why am I not getting the same number???#
#### STEP 2: RESULTS COLLAPSING ACROSS TRIAL TYPES#
#*$*trials completed#
results$trials_completed=as.numeric(table(data$subject_id)) #
#
#hist(data$object_touched_at)#
#hist(data$object_touched_at[data $object_touched_at<9000])#
#
## Exclusion criteria#
#maximum time to answer#
data[data $object_touched_at<7000,]-> data#
#
#dim(data) #4537 trials #
#
#*$* trials attempted#
results$trials_attempted=as.numeric(table(data$subject_id))#
#
#apply minimum amount of trials over the whole experiment#
bbMinOK=names(table(data $subject_id)[table(data $subject_id)>minTrialN])#
data[data $subject_id %in% bbMinOK,]-> data#
#
#dim(data) #4529 trials#
### ANALYSIS ACCURACY#
#*$* percent correct (overall)#
results$pc=extractVwithNA(levels(data$subject_id),	aggregate(data $correct,by=list(data $subject_id),mean))#
###XXXXX####
#Calculate exclusion matrix  #
exclude = table(data$subject_id,data$correct) < minTrialNtype   #Apply minimum number of trials #
#
### ANALYSIS RESPONSE TIMES #
results$rt_corr=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$correct==1],by=list(data $subject_id[data$correct==1]),median))#
results$rt_corr[exclude[,"1"]]<-NA#
#
results$rt_incorr=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$correct==0],by=list(data $subject_id[data$correct==0]),median))#
results$rt_incorr[exclude[,"0"]]<-NA#
#*$* for reliability calculations, re-do proportion correct & RT corr from odd and even trials#
data$spl=NA#
for(each_child in levels(data$subject_id)) if(sum(data$subject_id==each_child) == length(rep(1:2,sum(data$subject_id==each_child)/2))) data$spl[data$subject_id==each_child]<-rep(1:2, sum(data$subject_id==each_child)/2) else data$spl[data$subject_id==each_child]<-c(rep(1:2, sum(data$subject_id==each_child)/2),1)#
#
results$pc_1=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$spl==1],by=list(data $subject_id[data$spl==1]),mean))#
results$pc_2=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$spl==2],by=list(data $subject_id[data$spl==2]),mean))#
#
results$rt_1=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$spl==1],by=list(data $subject_id[data$spl==1]),median))#
results$rt_2=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$spl==2],by=list(data $subject_id[data$spl==2]),median))#
#
#*$* Write in the number of left responses#
results$propLeft=as.numeric(as.character(table(data$subject_id,data$object_touched_position)[,1]))/results$trials_attempted#
results$bias<-ifelse(results$propLeft <.4 | results$propLeft>.6,1,0)  #this should be improved!!#
#
#### STEP 3: RESULTS SEPARATING DIFFERENT TRIAL TYPES#
#
# Read in ancillary table#
dif=read.xlsx("../data/Difficultes.xlsx",1)#
#
## Add useful info to the data sheet#
#classify trials by word type & difficulty level#
data$dif=NA#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="easy"])]<-"easy"#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="moderate"])]<-"moderate"#
data$dif[as.character(data$object_asked) %in% as.character(dif$word[dif$level=="difficult"])]<-"difficult"#
data$dif=factor(data$dif)#
#
data$lex=NA#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="adj"])]<-"adj"#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="noun"])]<-"noun"#
data$lex[as.character(data$object_asked) %in% as.character(dif$word[dif$wordtype =="verb"])]<-"verb"#
#
#### by difficulty level#
#Ns#
results$N_easy=as.numeric(table(data$subject_id,data$dif)[,"easy"])#
results$N_mod=as.numeric(table(data$subject_id,data$dif)[,"moderate"])#
results$N_diff= as.numeric(table(data$subject_id,data$dif)[,"difficult"])#
#Calculate exclusion matrix#
exclude = table(data$subject_id,data$dif) < minTrialNtype   #Apply minimum number of trials #
#*$* percent correct#
results$pc_easy=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="easy"],by=list(data $subject_id[data$dif== "easy"]),mean))#
results$pc_easy[exclude[,"easy"]]<-NA#
#
results$pc_mod=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="moderate"],by=list(data $subject_id[data$dif== "moderate"]),mean))#
results$pc_mod[exclude[,"moderate"]]<-NA#
#
results$pc_diff=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$dif=="difficult"],by=list(data $subject_id[data$dif== "difficult"]),mean))#
results$pc_diff[exclude[,"difficult"]]<-NA#
#Calculate exclusion matrix#
exclude = table(data$subject_id[data$correct==1],data$dif[data$correct==1]) < minTrialNtype   #Apply minimum number of trials #
#
#*$* rt corr only#
results$rt_easy=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="easy" & data$correct==1],by=list(data $subject_id[data$dif== "easy" & data$correct==1]), median))#
results$rt_easy[exclude[,"easy"]]<-NA#
#
results$rt_mod=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="moderate" & data$correct==1],by=list(data $subject_id[data$dif== "moderate" & data$correct==1]), median))#
results$rt_mod[exclude[,"moderate"]]<-NA#
#
results$rt_diff=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$dif=="difficult" & data$correct==1],by=list(data $subject_id[data$dif== "difficult" & data$correct==1]),median))#
results$rt_diff[exclude[,"difficult"]]<-NA#
#### by lexical type#
#Ns#
results$N_noun=as.numeric(table(data$subject_id,data$lex)[,"noun"])#
results$N_adj=as.numeric(table(data$subject_id,data$lex)[,"adj"])#
results$N_verb=as.numeric(table(data$subject_id,data$lex)[,"verb"])#
#Calculate exclusion matrix#
exclude = table(data$subject_id,data$lex) < minTrialNtype#
#*$* percent correct#
results$pc_noun=extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="noun"],by=list(data $subject_id[data$lex== "noun"]),mean))#
results$pc_noun[exclude[,"noun"]]<-NA#
#
results$pc_adj =extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="adj"],by=list(data $subject_id[data$lex== "adj"]),mean))#
results$pc_adj[exclude[,"adj"]]<-NA#
#
results$pc_verb =extractVwithNA(levels(data$subject_id),aggregate(data $correct[data$lex =="verb"],by=list(data $subject_id[data$lex== "verb"]),mean))#
results$pc_verb[exclude[,"verb"]]<-NA#
#*$* rt corr only#
results$rt_noun=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="noun" & data$correct==1],by=list(data $subject_id[data$lex== "noun" & data$correct==1]), median))#
results$rt_noun[exclude[,"noun"]]<-NA#
#
results$rt_adj=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="adj" & data$correct==1],by=list(data $subject_id[data$lex== "adj" & data$correct==1]), median))#
results$rt_adj[exclude[,"adj"]]<-NA#
#
results$rt_verb=extractVwithNA(levels(data$subject_id),aggregate(data $object_touched_at[data$lex=="verb" & data$correct==1],by=list(data $subject_id[data$lex== "verb" & data$correct==1]),median))#
results$rt_verb[exclude[,"verb"]]<-NA#
#Squarify and write out#
resultsdf=data.frame(results)#
write.table(resultsdf,"../results/results.txt",row.names=F,quote=F,sep="\t")#
#
#And also write out the final data#
write.table(data,"../results/data_final.txt",row.names=F,quote=F,sep="\t")#
#
## add individual information#
read.table("../results/quest.txt",header=T)->quest#
read.table("../results/corresp.txt",header=T)->corresp#
#
merge(resultsdf,corresp,by.x="id",by.y="CSV")->resultsdf#
merge(resultsdf,quest[,1:5],by.x="Anon",by.y="CA")->resultsdf#
resultsdf$age=(as.Date(resultsdf$Date,"%d/%m/%Y") - as.Date(resultsdf$DOB,"%d/%m/%Y"))/365.25#
resultsdf<-resultsdf[resultsdf$Remove=="no",]#
resultsdf$ed=ifelse(resultsdf$edMom>7,"higher","lower")#
write.table(resultsdf,"../results/results.txt",row.names=F,sep="\t")
read.table("../results/results.txt",header=T)->data#
data$age.c=data$age-mean(data$age,na.rm=T)#
data$langdummy = data$lang=="monolingual"#
data$langdummy[is.na(data$langdummy)]<-F #LAIA, I think there is an issue with the bilingual category -- there should be an "other" (not mono, not bi)
rtstackcor=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_corr","rt_incorr")]))#
names(rtstackcor)[6:7]<-c("rt","resp")#
pcstackdif=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("pc_easy","pc_mod","pc_diff")]))#
names(pcstackdif)[6:7]<-c("pc","difficulty")#
#
rtstackdif=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_easy","rt_mod","rt_diff")]))#
names(rtstackdif)[6:7]<-c("rt","difficulty")#
#
pcstacklex=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("pc_noun","pc_adj","pc_verb")]))#
names(pcstacklex)[6:7]<-c("pc","lexCat")#
#
rtstacklex=cbind(data[,c("id","ed","age.c","langdummy","school")], stack(data[,c("rt_noun","rt_adj","rt_verb")]))#
names(rtstacklex)[6:7]<-c("rt","lexCat")
data$langdummy
print(date())#
print(dim(data))#
print(summary(data))
print("# ** planned analysis 1 **	total number of trials completed")#
#print(summary(lm(trials_completed ~ed+age.c,data=data))) #no difference across groups, marginal ages #
print(summary(lm(trials_completed ~ed+age.c+langdummy,data=data))) #all NS #
#
#plot(trials_completed ~age.c,data=data)#
#
print("trials_completed, mean & SD")#
print(cbind(c("Higher","Lower"),round(cbind(#
aggregate(data $trials_completed ,by=list(data$ed),mean)$x,#
aggregate(data $trials_completed ,by=list(data$ed),sd)$x),#
	3)))#
print("trials_completed, range")#
print(aggregate(data $trials_completed ,by=list(data$ed),range))#
print("# ** planned analysis 2 **	total number of trials attempted")#
#summary(lm(trials_attempted ~ed+age.c,data=data)) #no difference across groups, sign fx of age #
summary(lm(trials_attempted ~ed+age.c+langdummy,data=data)) #no difference across groups, sign fx of age #
#
print("trials_attempted, mean & SD")#
print(cbind(c("Higher","Lower"),round(cbind(#
aggregate(data $trials_attempted ,by=list(data$ed),mean)$x,#
aggregate(data $trials_attempted ,by=list(data$ed),sd)$x),#
	3)))#
#
print("trials_attempted, range")#
print(aggregate(data $trials_attempted ,by=list(data$ed),range))#
#
print("trials_attempted, distribution")#
print(table(data $trials_attempted,data$ed))#
#
print("# ** planned analysis 3 **	%correct & RT per group")#
#print(summary(lm((pc - .5)~ed+age.c,data=data)))  #difference in percent correct by age but not ed#
#print(summary(lm(log(rt_corr)~ed+age.c,data=data)))  #no difference in RT#
#
##### ATTENTION!!! THIS SECTION IS IN PROGRESS!!!#
mod1=glm(pc~ed+age.c+langdummy,data=data,family=binomial,weight= trials_attempted) #
mod2=glm(pc~edMom+age.c+langdummy,data=data,family=binomial,weight= trials_attempted) #
##### ATTENTION!!! END SECTION IN PROGRESS!!!#
#
print(summary(lm((pc - .5)~ed+age.c+langdummy,data=data)))  #difference in percent correct by age and language, but not ed#
print(summary(lm(log(rt_corr)~ed+age.c+langdummy,data=data)))  #no difference in RT#
print(summary(lm((pc - .5)~ed+age.c+langdummy+school,data=data)))  #difference in percent correct by age and language, but not ed#
print(summary(lm(log(rt_corr)~ed+age.c+langdummy+school,data=data)))  #no difference in RT#
print("# ** not planned **	RT per group * corr")#
mixedRTr=lmer(log(rt) ~ ed*resp+age.c + langdummy + (1|id),data= rtstackcor)  #marginal age & sig difficulty, but no ed & no interaction#
print(Anova(mixedRTr))#
summary(mixedRTr)#
print("# ** analysis 4 **	%correct & RT per group * difficulty (check - not sure if we declared it)#
")#
#mixedPCdif=lmer((pc - .5)~ed*difficulty+age.c + (1|id),data= pcstackdif) # difficulty, NO age, no interaction#
#print(Anova(mixedPCdif))#
#
mixedPCdif=lmer((pc - .5)~ed*difficulty+age.c + langdummy + (1|id),data= pcstackdif) #age & difficulty & langdummy, no interaction#
print(Anova(mixedPCdif))#
print(summary(mixedPCdif))#
#
mixedPCdif=lmer((pc - .5)~ed*difficulty+age.c + langdummy + school + (1|id),data= pcstackdif) #age & difficulty & langdummy, no interaction#
print(Anova(mixedPCdif))#
#mixedRTdif=lmer(log(rt) ~ ed*difficulty+age.c + (1|id),data= rtstackdif)  #marginal age & sig difficulty, but no ed & no interaction#
#print(Anova(mixedRTdif))#
#
mixedRTdif=lmer(log(rt) ~ ed*difficulty+age.c + langdummy + (1|id),data= rtstackdif)  #marginal age & sig difficulty, but no ed & no interaction#
print(Anova(mixedRTdif))#
print(summary(mixedRTdif))#
#
mixedRTdif=lmer(log(rt) ~ ed*difficulty+age.c + langdummy + school + (1|id),data= rtstackdif)  #marginal age & sig difficulty, but no ed & no interaction#
print(Anova(mixedRTdif))#
print("# ** analysis 5 **	compare different conceptualizations of ed (maternal education, etc.)")#
print("##TO DO")#
#
print("# ** analysis ** Differences by word type")#
#mixedPClex=lmer((pc - .5)~ed* lexCat +age.c + (1|id),data= pcstacklex) #age & lex cat, BUT no ed nor ed*lex interaction#
#print(Anova(mixedPClex))#
#
mixedPClex=lmer((pc - .5)~ed* lexCat +age.c + langdummy + (1|id),data= pcstacklex) #age & lex cat & dummylang, BUT no ed nor ed*lex interaction#
print(Anova(mixedPClex))#
print(summary(mixedPClex))#
#
mixedPClex=lmer((pc - .5)~ed* lexCat +age.c + langdummy + school + (1|id),data= pcstacklex) #age & lex cat & dummylang, BUT no ed nor ed*lex interaction#
print(Anova(mixedPClex))#
#
#mixedRTlex=lmer(log(rt) ~ ed* lexCat +age.c + (1|id),data= rtstacklex)  # lex cat, no ed or age and no interaction#
#print(Anova(mixedRTlex))#
#
mixedRTlex=lmer(log(rt) ~ ed* lexCat +age.c + langdummy+ (1|id),data= rtstacklex)  # lex cat & langdummy, no ed or age and no interaction#
print(Anova(mixedRTlex))#
print(summary(mixedRTlex))#
#
mixedRTlex=lmer(log(rt) ~ ed* lexCat +age.c + langdummy + school + (1|id),data= rtstacklex)  # lex cat & langdummy, no ed or age and no interaction#
print(Anova(mixedRTlex))#
#
print("# ** planned analysis **	Internal validity with cronbach alpha, using split trial") #
# (we had also planned for a retest reliability but we didn't include those extra 13 trials)#
# Santos, J. R. A. (1999). Cronbachs alpha: A tool for asedsing the reliability of scales. Journal of Extension, 37, 1–5.#
print(alpha(cbind(data$pc_1,data$pc_2)))#
print(alpha(cbind(data$rt_1,data$rt_2)))#
# ** minor analyses **	#
print("# Test potential presence of side bias")#
print(table(data$bias))  #19!!! out of 51 kids have a bias according to a crude measure that should be improved!!#
#
print("# ** NUTSHELL **	Effect sizes")#
dvs=c("trials_completed","trials_attempted","pc","rt_corr")#
for(thisdv in dvs){#
	print(paste(thisdv,"d=",r2d(cor.test(data $trials_completed , data $edMom, method="spearman")$est),"\n","r=",cor.test(data $trials_completed , data $edMom, method="spearman")$est))#
}
read.table("../results/results.txt",header=T)->data#
data$age.c=data$age-mean(data$age,na.rm=T)#
data$langdummy = data$lang=="monolingual"#
data$langdummy[is.na(data$langdummy)]<-F #LAIA, I think there is an issue with the bilingual category -- there should be an "other" (not mono, not bi)
mean(data$pc)
mean(data$pc,na.rm=T)
(mean(data$pc,na.rm=T)-.5)/sd(data$pc,na.rm=T)
